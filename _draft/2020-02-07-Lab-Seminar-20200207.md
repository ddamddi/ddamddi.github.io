---
layout: post
title: "[Lab Seminar] 2020-02-07 Lab Seminar"
description: ""
modified: 2020-02-07
categories: study
tags: paper
comments: true
---

이 글은 Lab Seminar 발표 논문을 정리한 글입니다.


### 0. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

#### Backgrounds
- RNN의 느린속도 개선
- RNN에서의 Long-Term dependency problem

#### Network
- Transformer
- Encoder : input word sequence -> vector
- Decoder : vector -> word sequence

#### What is attention?
- self attention
- multi-head attention

#### Encoder
- Residual connect 사용


## 1. [Fully Convolutional Networks for Semantic Segmentation](https://ieeexplore.ieee.org/document/7298965)

### Segmentations
- Sementic Segmentation (Dense Prediction, pixel-level classification)
- Instance Segmentation : Sementic Segmentation + Instance classification

### Fully Convolutional Network(FCN)
- 1*1 Convolution
    - 기존의 FCN를 사용하면 Spatial info가 사라짐
    - 1*1 Conv를 사용하면 Spatial info가 유지

- Transposed Convloution  
    - [Transposed Conv 사진]()
    - Upsampling
    - Deconvolution 이라고도 하는데 좋은 표현은 아님

### Architecture
- FC Layer 없이 Convolution + Transposed Convolution 으로만 이루어짐
- Deconv를 통해 Input image size와 같은 크기로 다시 키워주고 각각 pixel 단위 classification 수행
- FC-32 / FC-8
- [FC-32 사진]
- [FC-8 사진]

### Training
- Intilatization
- Loss
- IoU(Intersaction over Union)


## ~~2. [Prototypical Networks for few shot learning](https://arxiv.org/abs/1703.05175)~~

이해를 못했음.

## 참고자료

| Multi-Class AttentionNet, 2015  
| [1.퓨샷 러닝(few-shot learning) - Kakao Brain](https://www.kakaobrain.com/blog/106)